# CIFAR-10 模型调参指南：关键参数与影响分析

本文档整理了在 CIFAR-10 图像分类任务中，对模型性能（准确率与泛化能力）产生显著影响的关键参数及其调整建议。

## 1. 核心训练参数 (High Impact)

这些参数直接决定了模型的收敛能力和最终精度的上限，是调参中最先需要关注的部分。

### 1.1 优化器 (Optimizer)
*   **推荐调整**：从 `Adam` 切换为 `SGD` (Stochastic Gradient Descent)。
*   **配置建议**：
    *   `momentum=0.9`
    *   `weight_decay=5e-4`
    *   `lr=0.1` (作为基准初始学习率)
*   **原因分析**：
    *   虽然 `Adam` 在训练初期收敛速度快，但在 CIFAR-10 等图像分类任务上，配合动量和权重衰减的 `SGD` 通常能收敛到更平坦的极小值，从而获得更好的泛化能力（即测试集准确率更高）。
    *   `Adam` 有时会陷入局部最优，导致测试精度难以突破瓶颈（例如 LeNet 停滞在 60% 左右）。

### 1.2 学习率调度 (Learning Rate Scheduler)
*   **推荐调整**：引入动态学习率调整策略，如 `CosineAnnealingLR`。
*   **配置建议**：
    *   `T_max` = 总 Epochs 数
    *   或者使用 `MultiStepLR` 在特定 epoch（如 60%, 80% 处）衰减学习率。
*   **原因分析**：
    *   **前期**：较高的学习率帮助模型快速穿越平坦区，避免陷入浅层局部极小值。
    *   **后期**：随着训练进行，降低学习率能让模型在最优解附近微调，显著提升最终精度（通常能带来 1-3% 的提升）。
    *   恒定学习率容易导致模型在最优解附近震荡，无法进一步收敛。

### 1.3 训练轮数 (Epochs)
*   **推荐调整**：大幅增加训练轮数。
*   **配置建议**：
    *   **LeNet**: 从 10 增加到 50 ~ 100。
    *   **VGG**: 从 10 增加到 100 ~ 200。
*   **原因分析**：
    *   使用 `SGD` 优化器时，模型收敛速度较慢，需要更多的时间来探索参数空间。
    *   原始设置的 `10 epochs` 导致模型处于严重**欠拟合**状态，尚未发挥模型潜力。

---

## 2. 数据与正则化 (Data & Regularization)

这些参数主要用于缓解**过拟合 (Overfitting)**，即减小训练集精度与测试集精度之间的差距。

### 2.1 数据增强 (Data Augmentation)
*   **推荐调整**：对**训练集**启用随机变换。
*   **配置建议**：
    *   `transforms.RandomCrop(32, padding=4)`：随机裁剪，模拟物体位置变化。
    *   `transforms.RandomHorizontalFlip()`：随机水平翻转，利用图像的对称性。
*   **原因分析**：
    *   CIFAR-10 数据集较小（5万张训练图），模型极易记住样本。
    *   增强技术强迫模型学习物体的不变特征（如形状、纹理），而不是背景或特定像素位置，是提升泛化能力最有效的手段之一（通常提升 3-5%）。

### 2.2 标签平滑 (Label Smoothing)
*   **推荐调整**：在损失函数中启用。
*   **配置建议**：
    *   `nn.CrossEntropyLoss(label_smoothing=0.1)`
*   **原因分析**：
    *   防止模型对预测结果“过度自信”（即输出概率接近 1 或 0）。
    *   即使数据中有标注噪声，标签平滑也能提高模型的鲁棒性，使聚类更加紧凑。

### 2.3 权重衰减 (Weight Decay)
*   **推荐调整**：在使用 SGD 时设定非零值。
*   **配置建议**：`weight_decay=5e-4`
*   **原因分析**：
    *   相当于 L2 正则化，限制模型参数的数值大小，防止模型复杂度过高，从而抑制过拟合。

---

## 3. 模型结构微调 (Architecture Tweaks)

### 3.1 批归一化 (Batch Normalization, BN)
*   **现状**：`LeNet_Advanced` 和 `SimpleVGG` 已包含，`LeNet` 基础版未包含。
*   **影响**：
    *   **极大地加速收敛**：允许使用更大的学习率。
    *   **稳定性**：减少对参数初始化的敏感度。
    *   这也是为什么 LeNet Advanced 比基础版 LeNet 更容易训练且精度更高的核心原因。

### 3.2 Dropout
*   **现状**：`SimpleVGG` 在分类器层使用了 `Dropout(0.5)`。
*   **影响**：
    *   在深层网络或全连接层参数量大时，随机丢弃神经元连接是防止过拟合的强力手段。
    *   对于简单的 LeNet，如果数据增强足够强，Dropout 的边际收益可能减小。

---

## 4. 调参速查与预期效果表

| 参数项 | 原始设定 (基线) | 推荐调整方向 | 预期精度提升 | 备注 |
| :--- | :--- | :--- | :--- | :--- |
| **优化器** | Adam (lr=1e-3) | **SGD (lr=0.1, mom=0.9, wd=5e-4)** | ⭐⭐⭐ (+2~5%) | 需配合 Scheduler 使用 |
| **Epochs** | 10 | **50 - 200** | ⭐⭐⭐ (+5~15%) | 解决欠拟合最直接手段 |
| **数据增强** | 仅归一化 | **RandomCrop + Flip** | ⭐⭐⭐ (+3~6%) | 尤其是 LeNet 必做 |
| **LR Schedule** | 无 (Constant) | **CosineAnnealing** | ⭐⭐ (+1~3%) | 后期提分关键 |
| **Label Smooth** | 无 (0.0) | **0.1** | ⭐ (+0.5~1%) | 稳定提升，改动极小 |
| **Batch Size** | 64 | 64-128 | - | 只要不是过小(4)或过大(2048)影响不大 |

### 针对性调整策略
1.  **LeNet (基础版)**: 首先加上**数据增强**和增加 **Epoch**，效果立竿见影。
2.  **SimpleVGG**: 模型容量大，必须使用 **SGD + 权重衰减** 和 **Dropout** 来防止过拟合，同时需要更长的 **Epoch** 来训练。
