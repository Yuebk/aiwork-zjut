# 人工智能期末答辩“救命”速才表 —— 对抗式问答准备

**适用场景**：AI与Python导论期末答辩
**当前状态**：代码不是我写的，虽然跑通了但心里没底。
**战略目标**：苟过答辩，展现出“虽然代码可能是参考的，但我真的读懂了原理”的态度。重点防御老师最爱的“调参”问题。

---

## Q0: “请一句话概括你的工作。”
**回答**：
“我基于 PyTorch 框架复现了 LeNet 和 VGG 两种经典卷积神经网络，在 CIFAR-10 数据集上完成了从数据加载、模型构建到训练评估的完整流程，并重点探究了数据增强与优化器策略对模型准确率的影响。”

## Q0.5: “你觉得这个项目最难的地方在哪里？你是怎么解决的？”
*(这是个送分题，要展示你在这个过程中学到了东西)*
**回答**：
*   “最难的是**调参过程中的不确定性**。一开始用默认参数跑，准确率卡在 60% 上不去，我也很迷茫。”
*   “**解决**：我通过查阅资料（或者请教 GPT...啊不，查阅文献），了解到 SGD 配合动量在图像分类上表现更好，同时也尝试了数据增强。通过控制变量法，一次改一个参数，最后发现加上数据增强和延长 Epoch 效果最明显。这个过程让我深刻理解了理论知识在实际工程中的应用。”

---

## ⛔ 核心防御区：老师必问的“调参”灵魂拷问
*(老师上课反复提这个，这块必须背烂！)*

### Q1: “你这个模型准确率多少？为什么这么低（或这么高）？”
**回答思路**：先报数据，再由“过拟合/欠拟合”引出原因。
*   **话术**：
    *   “目前 LeNet 基础版在测试集上大概是 60% 左右，VGG 简单版能到 75%-80%。”
    *   “准确率没做到极致（90%+），主要受限于**算力**和**训练时间**。我目前的配置只跑了 10 个 Epoch，模型可能还处于**欠拟合**状态，如果在服务器上跑 100 个 Epoch，应该能更高。”

### Q2: “你说要提高准确率，具体你会怎么调参？（重点！）”
**回答思路**：分三步走：数据、模型、训练策略。不要只说一个。
*   **话术**：
    1.  **数据层面**：我会加**数据增强（Data Augmentation）**。现在的训练集只有 5 万张图，模型容易死记硬背。我会加随机裁剪（RandomCrop）和水平翻转（HorizontalFlip），强迫模型学习物体的不变性。
    2.  **优化器**：我现在默认用的是 Adam，它收敛快但泛化可能一般。如果给我更多时间，我会换成 **SGD + 动量 + 权重衰减**，虽然训练慢，但上限更高。
    3.  **学习率策略**：现在是固定学习率。我会加上 **Cosine Annealing（余弦退火）**，让学习率前期大（快速找路）、后期小（精细微调），这样能再抠出几个百分点的精度。

### Q3: “解释一下什么是过拟合（Overfitting），你的项目里有吗？怎么解决？”
**名词解释**：
*   **过拟合**：就像学生这道题背下来了，换个数就不会做了。表现为：**训练集 Loss 很低（全对），测试集 Loss 很高（不及格）**。
*   **回答**：
    *   “在 VGG 这种参数稍微多一点的模型上比较容易出现。解决办法主要是：1. **数据增强**（多做练习题）；2. **Dropout**（训练时随机让一些神经元不工作，防止它们抱团作弊/过度依赖）；3. **权重衰减/正则化**（限制参数不要太大，防止模型太极端）。”

### Q4: “什么是学习率（Learning Rate）？设太大或太小会怎样？”
**名词解释**：
*   **学习率**：就是模型下山找最优解时的“步长”。
*   **回答**：
    *   “如果设**太大**，模型会在山谷里跳来跳去，收敛不了（震荡），甚至直接飞出去（NaN）。
    *   如果设**太小**，下山比蜗牛还慢，训练时间超级长，而且可能卡在小坑里出不来（陷入局部最优）。”

### Q5: “你为什么选这个 Batch Size（比如 64）？对训练有什么影响？”
**回答**：
*   “选 64 或 128 主要是为了适应显存/内存大小。
*   **影响**：Batch Size 太小（比如 4），梯度计算波动大，有点像没头苍蝇乱撞，难以收敛；Batch Size 太大（比如几千），梯度虽然稳，但容易陷入局部最优，而且对显存要求高。64 是一个比较折中的经验值。”

### Q5.5: “你的模型是深层网络吗？为什么选这个深度的？”
**回答**：
*   “LeNet 比较浅，只有 5 层（2卷积+3全连接），适合 CIFAR-10 这种小图入门。SimpleVGG 稍微深一点，仿照了 VGG 的 Block 结构（几个卷积+池化堆叠），大概有 10 层左右。”
*   “**深度选择原因**：CIFAR-10 只是 32x32 的小图，太深的网络（比如 ResNet-152）可能会**梯度消失**或者严重**过拟合**，而且训练太慢。对于我的算力来说，Lenet 和简化版 VGG 性价比最高。”

### Q5.6: “如果训练 Loss 不降了（Loss 震荡），你觉得是什么原因？怎么解决？”
**回答**：
*   “**原因**：很可能是**学习率太大**了，导致模型在坑边缘跳来跳去进不去；或者 Batch Size 太小导致梯度乱跑。”
*   “**解决**：我会尝试**降低学习率**（比如除以 10），或者使用我之前提到的 Cosine Annealing 学习率调度器。如果是 Batch Size 太小，就在显存允许的情况下调大一点。”

---

## 🧠 基础概念区：防止被老师问甚至不懂名词

### Q6: “介绍一下你的项目结构/流程。”
**回答**：
“我的项目是一个基于 CIFAR-10 数据集的图像分类任务。主要分为三个部分：
1.  **`data_loader.py`**：负责把图片读进来，转成 PyTorch 能认的 Tensor 格式，如果是训练集还会做点随机变形（增强）。
2.  **`model.py`**：定义了网络结构，我对比了简单的 LeNet 和稍微深一点的 SimpleVGG。
3.  **`main.py / train_eval.py`**：这就是训练的‘大循环’。在这个循环里，先把图片喂给模型，算出误差（Loss），然后求导（Backward），最后更新参数（Step）。”

### Q7: “什么是卷积（Convolution）？为什么不用全连接层直接做？”
**回答**：
*   “**卷积**就是用一个小窗口（比如 3x3 像素）在图片上滑来滑去，提取局部特征（比如边缘、线条）。
*   不用全连接是因为：1. **参数太多炸了**（全连接要把所有像素都连起来）；2. **丢失空间信息**（全连接会把图片拉成一条线，它就不知道眼睛在眉毛下面了）。卷积能保留空间结构，而且参数共享，比较省资源。”

### Q8: “什么是池化（Pooling）？有什么用？”
**回答**：
*   “池化就是‘降采样’。比如 2x2 的 Max Pooling，就是把 2x2 的格子里最大的那个数挑出来。
*   **作用**：1. 把图片变小，减少计算量；2. 提取最显著的特征（最大的那个数），不用管位置稍微偏了一点点这种细节（平移不变性）。”

### Q9: “Activation Function（激活函数）用的什么？为什么要用它？”
**回答**：
*   “我主要用了 **ReLU**。哪怕没有代码，我也知道它是 `max(0, x)`，小于 0 变成 0，大于 0 保持不变。
*   **为什么要用**：如果不加激活函数，多少层神经网络堆起来都只相当于一层线性变换（矩阵乘法）。加了非线性的 ReLU，神经网络才能去拟合那种弯弯曲曲的复杂分类边界。”

### Q9.5: “什么是 Dropout？它原理是什么？”
**回答**：
*   “Dropout 是一种正则化手段。简单说就是**每次训练随机‘拔掉’一些神经元的网线**（比如设 0.5 的概率置零）。”
*   “**原理**：
    1.  防止神经元之间‘搞小团体’（共适应），强迫每个神经元都要能独立提取特征。
    2.  相当于训练了很多个不同结构的子网络，测试的时候再合起来，有一种‘模型集成’（Ensemble）的效果。”
*   “**注意**：测试（Eval）的时候一定要关掉 Dropout（`model.eval()` 会自动处理），因为测试时我们需要所有神经元全力以赴。”

### Q9.6: “什么是 BN (Batch Normalization/批归一化)？为什么它能加速训练？”
**回答**：
*   “BN 就是在每一层激活函数之前，把传过来的数据强行拉回到均值为 0、方差为 1 的正态分布。”
*   “**作用**：
    1.  解决**内部协变量偏移（Internal Covariate Shift）**的问题（这就显得很专业了）。通俗说就是防止后面层的输入数据分布老是变来变去，后面层就不用总是重新适应前面层。
    2.  因为它把数据都拉回了比较标准的范围，梯度传导更顺畅，所以可以用**更大的学习率**，收敛就更快了。”

---

## ⚔️ 进阶对抗区：如果老师盯着代码问

### Q10: “我看你用了 SGD，什么是随机梯度下降？”
*(参考之前的文档，这里用大白话)*
**回答**：
“就是不一次性把几万张图都算一遍梯度（太慢），而是每次随机抓一小把（Mini-batch）来算方向。虽然每一步走得有点晃悠（随机性），但总体方向是对的，而且这种晃悠有时候能帮模型跳出小坑。”

### Q11: “代码里的 `optimizer.zero_grad()` 是干嘛的？删了行不行？”
**回答**：
“**绝对不能删！** PyTorch 默认会把梯度‘累加’起来。如果不清零，这一轮算的梯度会和上一轮的梯度加在一起，方向就全乱了，模型根本没法训练。”

### Q12: “你的 loss function 选的什么？为什么？”
**回答**：
“用的 **CrossEntropyLoss（交叉熵损失）**。因为这是分类任务。交叉熵是专门用来衡量‘两个概率分布有多像’的。它可以衡量我们预测的概率（比如 0.1, 0.8, 0.1）和真实标签（0, 1, 0）之间的距离。”

### Q13: “`model.train()` 和 `model.eval()` 有什么区别？如果不写会怎样？”
**回答**：
*   “**区别**：这两个模式开关主要影响 **Dropout** 和 **BatchNorm** 层。
    *   `train()` 模式下：Dropout 随机丢弃，BN 会更新正在统计的均值和方差。
    *   `eval()` 模式下：Dropout 不工作（全通），BN 使用之前训练好的统计数据，不再更新。”
*   “**不写的后果**：如果在测试时忘了写 `model.eval()`，Dropout 还会继续随机丢数据，导致测试出来的准确率比实际低，而且很不稳定；BN 也会继续根据测试集的 Batch 更新统计量，这在原则上是不对的。”

### Q14: “你用 GPU 还是 CPU 跑的？代码里怎么体现的？”
**回答**：
*   “我用了 GPU（如果有的话）。代码里是通过 `.to(device)` 来体现的。”
*   “**细节**：要注意模型（`model.to(device)`）和数据（`inputs.to(device)`）都要搬到显卡上，否则会报错。PyTorch 不支持 CPU Tensor 和 GPU Tensor 直接做运算。”

### Q15: “你的数据预处理里的 `Normalize` 参数是怎么来的？”
**回答**：
*   “那个 `stats = ((0.4914...),(0.2023...))` 是 CIFAR-10 这个公开数据集通用的均值和标准差。”
*   “**作用**：是为了把输入的图片像素值（原本是 0-255 或者 0-1）拉回到以 0 为中心。这样能让数据分布更对称，有助于优化器更快地找到梯度下降的方向，加速收敛。”

---

## 🆘 应急术语表（背几个高大上的词防身）

1.  **Epoch（轮）**：全套教材学一遍。
2.  **Iteration/Step（步）**：做了一次练习题（处理了一个 Batch）。
3.  **Tensor（张量）**：PyTorch 里的“矩阵”，不仅存数，还带着在这个数上怎么求导的信息。
4.  **Backpropagation（反向传播）**：一种算法，根据算出来的误差，倒回去告诉前面的每一层参数：“你该改多少”。
5.  **Softmax**：放在模型最后面的，把一堆乱七八糟的输出数值，变成加起来等于 1 的“概率”。

---

## 🚩 假如完全不会答怎么办？

*   **万能句式 A**：“这个问题确实值得深究，在目前的**算力限制**下我主要关注了 X，如果后续有资源，我会尝试 Y 方法来验证您说的这个问题。”
*   **万能句式 B**：“这部分代码采用了 PyTorch 的标准实现，我的理解侧重于它在任务中的宏观表现，具体的底层数学推导我不仅在课上学过，在实验中也观察到了它的收敛特性（虽然其实没太看懂）。”
