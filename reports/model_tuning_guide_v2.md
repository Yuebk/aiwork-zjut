# CIFAR-10 模型调参指南：关键参数与深度解析

本文档整理了在 CIFAR-10 图像分类任务中，对模型性能（准确率与泛化能力）产生显著影响的关键参数。为了帮助更好地理解，我们对每个技术名词进行了详细的原理解释。

## 1. 核心训练参数 (High Impact)

这些参数直接决定了模型的收敛能力和最终精度的上限，是调参中最先需要关注的部分。

### 1.1 优化器 (Optimizer)
*   **推荐调整**：从 `Adam` 切换为 `SGD` (Stochastic Gradient Descent)。
*   **什么是 SGD (随机梯度下降)？**
    *   **定义**：全称 Stochastic Gradient Descent。它不使用所有样本来计算梯度（那样太慢），而是每次随机抽取一小批样本（Mini-batch）来估算梯度的方向，然后更新模型参数。
    *   **比喻**：想象你在大雾中下山。
        *   **Adam**：装备了高科技导航，能根据之前的坡度自动调整步伐大小。它下山非常快，但有时候导航会因为初期的地形误判，把你带到一个小山沟里（局部最优解），而不是真正的山脚（全局最优解）。
        *   **SGD**：像是徒步者。虽然步伐可能有些摇晃（随机性），但它配合“动量”机制，更有机会冲出那些浅浅的小山沟，最终找到更深的山谷。
*   **为何在图像分类中 SGD 优于 Adam？**
    *   虽然 Adam 训练初期下降极快，但在 CV 任务中，经过精细调节的 SGD（配合动量和权重衰减）通常能收敛到更“平坦”的极小值区域。平坦区域意味着：当测试数据与训练数据稍有偏差时，模型的误差不会剧烈波动，即**泛化能力更强**。
*   **配置建议**：
    *   `optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)`
    *   **Momentum (动量, 0.9)**：模拟物理惯性。如果之前的梯度方向是向下的，现在的梯度也是向下的，那就加速；如果方向变了，利用惯性保持平滑。这能抑制震荡，加速收敛。

### 1.2 学习率调度 (Learning Rate Scheduler)
*   **推荐调整**：引入动态调整策略，如 `CosineAnnealingLR` (余弦退火)。
*   **什么是学习率 (Learning Rate, LR)？**
    *   这是模型更新参数的“步伐大小”。
    *   **太大**：步子扯大了，容易跨过最低点，甚至越走越高（震荡发散）。
    *   **太小**：像蚂蚁搬家，训练极其缓慢，且容易卡在半山腰下不去。
*   **什么是余弦退火 (Cosine Annealing)？**
    *   一种让学习率随时间按余弦曲线变化的策略。
    *   **前期**：步伐较大，快速探索，翻山越岭寻找大概的最低点区域。
    *   **后期**：步伐变得非常小，在找到的最低点区域内进行精细的微调。
    *   这种“先快后慢”的策略比一直用固定的步长（Constant）效果好得多。
*   **配置建议**：
    *   `scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)`

### 1.3 训练轮数 (Epochs)
*   **推荐调整**：大幅增加训练轮数（从 10 增加到 50~200）。
*   **什么是 Epoch？**
    *   1 个 Epoch 意味着模型把训练集中所有的图片完整地看了一遍。
    *   **欠拟合 (Underfitting)**：就像考试复习，书还没看完一半就上考场了。如果 Epoch 太少（例如只有 10），使用 SGD 这种收敛稍慢的优化器时，模型往往还没学透，处于欠拟合状态。
*   **配置建议**：
    *   **LeNet**: 50 ~ 100 Epochs。
    *   **VGG**: 100 ~ 200 Epochs (深层网络参数更多，需要更久的时间来训练)。

---

## 2. 数据与正则化 (Data & Regularization)

这些参数主要用于缓解**过拟合 (Overfitting)**。
**过拟合**是指模型就像只会死记硬背的学生，做过的练习题（训练集）全对，但一碰到新题（测试集）就不会了。

### 2.1 数据增强 (Data Augmentation)
*   **推荐调整**：对**训练集**启用随机变换。
*   **具体手段**：
    *   **RandomCrop (随机裁剪)**：在图片周围填充一圈像素，然后随机切一块出来。这让模型明白：物体即使稍微偏左一点、偏右一点，它还是一只猫。
    *   **RandomHorizontalFlip (随机水平翻转)**：这让模型明白：猫头朝左看是猫，朝右看也还是猫。
*   **原理**：
    *   CIFAR-10 数据集只有 5 万张图。通过这些变换，相当于我们在不增加采集成本的情况下，让训练数据呈指数级“变多”了。这强迫模型去学习物体的本质特征（如猫的耳朵形状），而不是死记硬背某张图第几个像素是什么颜色。

### 2.2 权重衰减 (Weight Decay)
*   **推荐调整**：`weight_decay=5e-4`。
*   **原理 (L2 正则化)**：
    *   在训练过程中，我们在损失函数里加了一项关于“参数数值大小”的惩罚。
    *   如果不加限制，模型为了迎合某些极端的训练样本，可能会把某些神经元的权重变得极大。权重衰减就像给模型加了“紧箍咒”，迫使它倾向于用更小、更分散的权重来拟合数据。
    *   参数数值越小，模型函数越平滑，对输入的变化就不那么敏感，从而**提升泛化能力**。

### 2.3 标签平滑 (Label Smoothing)
*   **推荐调整**：`label_smoothing=0.1`。
*   **原理**：
    *   通常我们告诉模型：这张图**绝对**是猫（概率 1.0），**绝对**不是狗（概率 0.0）。这叫 One-hot 编码。
    *   但这太绝对了。如果标注员手抖标错了呢？或者这张图确实很模糊呢？
    *   标签平滑告诉模型：这张图**很可能**是猫（概率 0.9），但也留一点点可能性是别的（0.1 分给其他类）。
    *   这防止了模型变得“盲目自信”，使学到的特征更加鲁棒。

---

## 3. 调参速查与预期效果表

| 参数项 | 原始设定 | 推荐调整方向 | 解释摘要 |
| :--- | :--- | :--- | :--- |
| **优化器** | Adam | **SGD** | 虽然慢但稳，能找到更好的最优解，**提升上限**。 |
| **学习率** | 恒定 | **Cosine退火** | 先快后慢，后期微调提分关键。 |
| **Epochs** | 10 | **50 - 200** | 给模型足够的时间去“复习”，解决欠拟合。 |
| **数据增强** | 无 | **Crop + Flip** | 让模型不再死记硬背，学会举一反三。 |
| **标签平滑** | 无 | **0.1** | 防止盲目自信，提升抗干扰能力。 |

### 针对性调整建议
*   **如果你想最快看到效果**：直接把 `Epoch` 加到 50，并加上 `数据增强`。
*   **如果你追求极致高分**：必须换用 `SGD`，且配合 `Cosine` 学习率调度和 `权重衰减`，耐心训练更长时间。
